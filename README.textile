h1. Tracking Trends with Hadoop & Hive on EC2

This repository contains the full source code for "Trendingtopics.org":http://www.trendingtopics.org, built by "Data Wrangling":http://www,datawrangling.com to demonstrate how Hadoop & EC2 can power a data driven website. The trend statistics and time series data that run the site are updated periodically by launching a temporary EC2 cluster running the "Cloudera Hadoop Distribution":http://www.cloudera.com/hadoop-ec2. Our initial seed data consists of the raw wikipedia database content dump along with hourly "traffic logs":http://stats.grok.se/ for all articles collected from the wikipedia squid proxy (curated by Domas Mituzas). We made the first 7 months of this hourly data for all articles available as an Amazon Public Dataset.

The current trend calculations are done using Hadoop Streaming and Hive.  The output produced by these Hadoop jobs are loaded into MySQL and indexed to power the live site. The demo data included with the Rails app on Github was generated from a sample of 100 trending articles on June 6th 2009. A larger snapshot is available on Amazon Public Datasets (snap-753dfc1c). The Rails app and MySQL database are deployed on Amazon EC2 using Paul Downman’s "EC2onRails":http://ec2onrails.rubyforge.org/.

!http://trendingtopics.s3.amazonaws.com/images/trendingtopics_dashboard.png!

h3. Application Features

* Ranked list of the most significant trends over the last 30 days along with total pageviews
* Ranked list of "Rising" articles trending in the last 24 hours
* Daily time series charts and sparklines for over 2.5 Million wikipedia articles
* Autocomplete functionality and search results ranked by article trend score

h3. How Hadoop is Used in the Application

* Cleaning raw log data and joining title strings with wikipedia page ids
* Aggregating hourly time series data for daily pageview charts and sparklines 
* Generating Statistics that power search autocomplete and the ranking of search results
* Running periodic trend estimation jobs / regressions

The rest of this document will walk through the code which powers the site and describe some basic approaches for extracting trends from log data with Hadoop and Hive.

h2. Raw Data: Hourly Wikipedia Article Traffic Logs

The "Wikipedia Traffic Statistics":http://developer.amazonwebservices.com/connect/entry.jspa?externalID=2596 Amazon Public Dataset we will process covers a 7 month period from October, 01 2008 to April, 30 2009. This data is regularly logged from the wikipedia squid proxy by Domas Mituzas and was compiled by Data Wrangling for this demo.

h3. Log file format

Each log file is named with the date and time of collection: <code>pagecounts-20090430-230000.gz</code>. The individual hourly files are around 55 MB when compressed, so 8 months of compressed data takes up about 300 GB of space.

Each line has 4 fields: <code>projectcode, pagename, pageviews, bytes</code>

<pre>
      $ grep '^en Barack' pagecounts-20090521-100001 
      en Barack 8 1240112
      en Barack%20Obama 1 1167
      en Barack_H._Obama 1 142802
      en Barack_H_Obama 3 428946
      en Barack_H_Obama_Jr. 2 285780
      en Barack_Hussein_Obama,_Junior 2 285606
      en Barack_O%27Bama 1 142796
      en Barack_Obama 701 139248439
      en Barack_Obama%27s_first_100_days 2 143181
      en Barack_Obama,_Jr 2 285755
</pre>

Many of the raw wiki log page titles are percent-encoded. To match these with the page titles in the wikipedia database, you can transform them as follows in Python:

<pre>
      $ python
     >>> import urllib
     >>> escaped_title = '%22Klondike_Kate%22_Rockwell'
     >>> print urllib.unquote_plus(escaped_title)
     "Klondike_Kate"_Rockwell
</pre>


h3. Handling Wikipedia Redirects

To further complicate the raw data processing, many page titles are actually wikipedia redirects which do not match to the primary key of the wikipedia "Pages" table.  The non-redirect articles are referred to as belonging to "namespace-0".  As part of our Hadoop pre-processing, we can perform a join against a redirect lookup table to find the true wikipedia pageid for a give article title.

<pre>
   mysql> select * from page_lookups where page_id = 534366;
   +---------+------------------------------------------------+--------------+---------+-------------+
   | id      | redirect_title                                 | true_title   | page_id | page_latest |
   +---------+------------------------------------------------+--------------+---------+-------------+
   |  219291 | Barack_Obama                                   | Barack Obama |  534366 |   276223690 | 
   | 3151538 | Barak_Obama                                    | Barack Obama |  534366 |   276223690 | 
   | 3151543 | 44th_President_of_the_United_States            | Barack Obama |  534366 |   276223690 | 
   | 3151544 | Barach_Obama                                   | Barack Obama |  534366 |   276223690 | 
   | 3151545 | Senator_Barack_Obama                           | Barack Obama |  534366 |   276223690 | 
      ....                                                                                   ....

   | 3151644 | Rocco_Bama                                     | Barack Obama |  534366 |   276223690 | 
   | 3151645 | Barack_Obama's                                 | Barack Obama |  534366 |   276223690 | 
   | 3151646 | B._Obama                                       | Barack Obama |  534366 |   276223690 | 
   +---------+------------------------------------------------+--------------+---------+-------------+
   110 rows in set (11.15 sec)    
</pre>


h2. Using Hadoop & Hive on EC2 to Extract Trends

Processing large log datasets like this is a typical use case for Hadoop.  By using MapReduce and EC2 we can run millions of regressions on a distributed cluster to quickly answer analytical questions or generate real time analytics on metrics of interest.

h3. Task 1: Generate Daily Timelines from 1TB of Historical Hourly Logs

Our first task is to calculate daily page views for all 2.5 million English Wikipedia articles, so we can display dynamic Google finance style timelines on the site.

!http://trendingtopics.s3.amazonaws.com/images/american_idol_daily_pageviews.png!

First, we need to access the Amazon public dataset.  From your local machine, launch a small EC2 Ubuntu instance:  

<pre>
	skom:~ pskomoroch$ ec2-run-instances ami-5394733a -k gsg-keypair -z us-east-1a
</pre>

Once it is running and you have the instance id, create and attach an EBS Volume using the Wikipedia Traffic Statistics "public snapshot":http://developer.amazonwebservices.com/connect/entry.jspa?externalID=2596 (make sure the volume is created in the same availability zone as the ec2 instance)

<pre>
	skom:~ pskomoroch$ ec2-create-volume --snapshot snap-753dfc1c -z us-east-1a
	skom:~ pskomoroch$ ec2-attach-volume vol-ec06ea85 -i i-df396cb6 -d /dev/sdf
</pre>

Next, ssh into the instance and mount the volume

<pre>
	skom:~ pskomoroch$ ssh root@ec2-12-xx-xx-xx.z-1.compute-1.amazonaws.com
	root@domU-12-xx-xx-xx-75-81:/mnt# mkdir /mnt/wikidata
	root@domU-12-xx-xx-xx-75-81:/mnt# mount /dev/sdf /mnt/wikidata
</pre>	
	
h4. Uploading the data to S3	
	
Hadoop can use Amazon S3 as a distributed file system out of the box, so it will be easier for future EC2 jobs if we store a full copy of the EBS data on S3.  We will copy the raw pagecount data up to S3 from the EBS volume using S3cmd.  This install and configuration can be automated later on, but for now you will need your AWS access keys handy for the configuration.

<pre>
	root@domU-12-xx-xx-xx-75-81:/mnt# apt-get install -y s3cmd
	root@domU-12-xx-xx-xx-75-81:/mnt# s3cmd --configure
</pre>

You can upload all the files with a single command, or select a subset based on the file name in case an upload fails:

<pre>
	/mnt# time s3cmd put --force pagecounts-200904* s3://trendingtopics/wikistats/
	/mnt# time s3cmd put --force pagecounts-200903* s3://trendingtopics/wikistats/
	/mnt# time s3cmd put --force pagecounts-200902* s3://trendingtopics/wikistats/
	/mnt# time s3cmd put --force pagecounts-200901* s3://trendingtopics/wikistats/
	/mnt# time s3cmd put --force pagecounts-200812* s3://trendingtopics/wikistats/
	/mnt# time s3cmd put --force pagecounts-200811* s3://trendingtopics/wikistats/
	/mnt# time s3cmd put --force pagecounts-200810* s3://trendingtopics/wikistats/	
	....
</pre>	

Each month has approximately 40GB of compressed data and takes around 30 minutes tt upload from a small EC2 instance.  Data transfer between EC2 and S3 is free so it costs about 50 cents do do the entire one time upload.

h4. Customize the Cloudera Hadoop Ubuntu launch scripts


... add packages for s3cmd, git, Rpy, etc


I found the following settings to work well for c1.medium instances on EC2.  This will likely vary based on the nature of your Hadoop Job.  

<pre>
	MAX_MAP_TASKS=2
	MAX_REDUCE_TASKS=1

	real	22m41.951s
	user	0m1.924s
	sys	0m0.276s


	MAX_MAP_TASKS=4
	MAX_REDUCE_TASKS=2

	real	15m47.388s
	user	0m2.948s
	sys	0m0.324s	
</pre>	

For c1.xlarge instances, I used <code>MAX_MAP_TASKS=8</code> and <code>MAX_REDUCE_TASKS=4</code>.  

h4. Launch an EC2 Hadoop Cluster

For running daily timeline aggregations across 8 months of data, we will start a 10 node c1.xlarge Hadoop cluster

<pre>
	skom:cloudera-for-hadoop-on-ec2-0.3.1 pskomoroch$ bin/hadoop-ec2 launch-cluster my-hadoop-cluster 10
	skom:cloudera-for-hadoop-on-ec2-0.3.1 pskomoroch$ ssh root@ec2-174-129-165-138.compute-1.amazonaws.com
	$ cd /mnt	
</pre>		
	
Once logged into the master node, you need to wait for the file system setup to complete before running the Hadoop & Hive Jobs.  A "hadoop" directory will appear in /mnt on the master node when the cluster is ready.  While you are waiting, configure S3cmd on the master node as we did previously when uploading the raw data to S3.

When the file system is ready, fetch the trendingtopics code from github and then start the daily timeline job:

<pre>
	$ git clone git://github.com/datawrangling/trendingtopics.git
	$ bash trendingtopics/lib/scripts/run_daily_timelines.sh
</pre>

While the job is running, we can dig into the MapReduce code to see how it does the aggregation and data cleaning:

------------------------


TODO:  What we are running: Python Streaming snippets and Hive Join

Use filename in mapper trick, show hadoop tricks /tips

TODO: give more details on the Hadoop / Hive jobs along with code snippets.
 We are only using Hive here for some simple Joins and selects,
 but it has other powerful features for analytics...


------------------------

You can monitor your job progress by using Foxy Proxy to view the Cloudera Hadoop Web UI (see the README included with the Cloudera EC2 scripts for more details).  The following command sets up a tunnel for the Web UI.

<pre>
	$ ssh -ND 8157 root@ec2-174-129-165-138.compute-1.amazonaws.com	
</pre>	


When the Hive job completes, we can inspect the results:

<pre>
	$ hive
	SELECT COUNT(1) FROM raw_daily_stats_table; 
	2823525
	Time taken: 33.199 seconds
</pre>

Some other simple queries you can try:

<pre>
	SELECT MAX(total_pageviews) FROM raw_daily_stats_table;
	+----------------------+
	| MAX(total_pageviews) |
	+----------------------+
	|             33471178 | 
	+----------------------+
	
	SELECT redirect_title from raw_daily_stats_table SORT BY total_pageviews DESC LIMIT 10;
	+----------------+
	| redirect_title |
	+----------------+
	| Wiki           | 
	| The_Beatles    | 
	| Barack_Obama   | 
	| YouTube        | 
	| Wikipedia      | 
	| United_States  | 
	| Facebook       | 
	| Deaths_in_2009 | 
	| Eminem         | 
	| World_War_II   | 
	+----------------+
	
	SELECT redirect_title from raw_daily_stats_table SORT BY monthly_trend DESC LIMIT 10;
	+-----------------------------------+
	| redirect_title                    |
	+-----------------------------------+
	| David_Carradine                   | 
	| Sonia_Sotomayor                   | 
	| North_Korea                       | 
	| Tiananmen_Square_protests_of_1989 | 
	| Land_of_the_Lost_(1974_TV_series) | 
	| Britain's_Got_Talent              | 
	| Lady_Gaga                         | 
	| Bermuda_Triangle                  | 
	| Mike_Tyson                        | 
	| Up_(2009_film)                    | 
	+-----------------------------------+
</pre>

Along with powering the ranked lists of trending articles on the site, these rankings are used to rank autocomplete search results:

!http://trendingtopics.s3.amazonaws.com/images/trend_autocomplete.png!


Once we are confident in the results of the Hadoop jobs, we can send the results over to the trendingtopics database server:

<pre>
	$ scp /mnt/trendsdb.tar.gz root@www.trendingtopics.org:/mnt/
</pre>

We also send a copy of the full data up to S3 for safe keeping along with the sample data for development use

<pre>
	$ s3cmd put trendsdb.tar.gz s3://trendingtopics/archive/trendsdb.tar.gz
	$ s3cmd put --force /mnt/sample* s3://trendingtopics/sampledata/	
</pre>

Jump over to the database server and execute the <code>load_history.sql</code> script against <code>trendingtopics_production</code>.  You will want to automate these logins and use proper authentication for your own app instead of manually loading the production database like this.  

<pre>
	$ ssh root@www.trendingtopics.org 
	$ cd /mnt
	$ tar -xzvf trendsdb.tar.gz
	$ mysql -u root trendingtopics_production < load_history.sql
</pre>

h3. Task 2: Using Hadoop and Hive for Exploratory Analysis

--------------------------

TODO
...

-------------------------
!http://trendingtopics.s3.amazonaws.com/images/fit_example_small.png!



h3. Task 3: Daily Trend Estimation

!http://trendingtopics.s3.amazonaws.com/images/monthly_and_daily_trends.png!

Repeat the initial cluster launch and configuration steps used in the last job.  This time, we run <code>daily_trend.sh</code>:

<pre>
	$ cd /mnt
	$ bash bash trendingtopics/lib/scripts/run_daily_trends.sh
</pre>

-----------
TODO:

Here is what the script is executing:

Use regex on input bucket trick

Show cron job to fetch and upload files to s3


This process is fairly similar to the first historical aggregation, except we pass the hourly data to the trend estimation script

Python Streaming example, pulling input data from S3
------------

We can inspect a sample of the results before loading them into MySQL:

<pre><code>
	SELECT redirect_table.redirect_title, 
		raw_daily_trends_table.trend, 
		raw_daily_trends_table.error 
	FROM redirect_table JOIN raw_daily_trends_table 
	ON (redirect_table.redirect_title = raw_daily_trends_table.redirect_title) 
	SORT BY trend DESC LIMIT 100;
</code></pre>

The raw trends are also sitting in the "finaltrendoutput" directory on hdfs after the job runs

<pre>
	$ hadoop fs -cat finaloutput/part-00001 | head
	Ant_&_Dec	-2306.54138876	0.00584615412289
	Antarctic_Peninsula	3.25105235217	0.022530295453
	Ante_Paveliƒá	43.0203206758	0.0190174565282
	Antebellum	181.963189695	0.00773800065214
</pre>

Next we send copies of the trend file & sample data up to Amazon S3 for development use later on

<pre>
	s3cmd put /mnt/daily_trends.txt s3://trendingtopics/archive/20090606/daily_trends.txt
	s3cmd put --force /mnt/sample_daily_trends.txt s3://trendingtopics/sampledata/sample_daily_trends.txt
</pre>

Copy the output file over to the trendingtopics server:

<pre>
scp /mnt/daily_trends.txt root@www.trendingtopics.org:/mnt/
</pre>

Load the daily trend data into MySQL on the prod server and build the trend indices:

<pre>
	ssh root@www.trendingtopics.org 
	cd /mnt
	mysql -u root trendingtopics_production < load_trends.sql
</pre>

Shut down the Hadoop cluster

<pre>	
	$ bin/hadoop-ec2 terminate-cluster my-hadoop-cluster
	Terminate all instances? [yes or no]: yes
	INSTANCE	i-2d7b2c44	running	shutting-down
	INSTANCE	i-eb7b2c82	running	shutting-down
	INSTANCE	i-ed7b2c84	running	shutting-down
	INSTANCE	i-ef7b2c86	running	shutting-down
	INSTANCE	i-e17b2c88	running	shutting-down
	INSTANCE	i-e37b2c8a	running	shutting-down
	INSTANCE	i-e57b2c8c	running	shutting-down
	INSTANCE	i-e77b2c8e	running	shutting-down
	INSTANCE	i-f97b2c90	running	shutting-down
	INSTANCE	i-fb7b2c92	running	shutting-down
	INSTANCE	i-fd7b2c94	running	shutting-down
</pre>	

h2. Next Steps with MapReduce and TrendingTopics

* Cron / Rake tasks to automate the daily job
* Plug in better trend algorithms
* Explore Paritions/Buckets with Hive for fast analytics queries
* Generate smaller representative sample datasets for R&D with Hive 
* Merge the trends with text content from the raw wikipedia dumps using Hive to get trends for words/phrases/concepts 
* Combine data with wikipedia link graph dataset on the EBS volume, show related articles for each trend
* Find correlated trends and topics, look for predictive value

h2. Appendix: Running the Rails app

h3. Dependencies for local development:

* Ruby (1.8.7)
* Ruby Gems (1.3.1)
* Capistrano (v2.5.5)
* Rails (2.3.2)

h3. Additional dependencies for running on EC2:

* "Amazon EC2 Account":http://aws.amazon.com/ec2/ 
* Steps from EC2 "Getting Started Guide":http://docs.amazonwebservices.com/AWSEC2/latest/GettingStartedGuide/
* "EC2onRails":http://ec2onrails.rubyforge.org/
* "Cloudera EC2 Hadoop scripts":http://www.cloudera.com/hadoop-ec2
* "1 TB of Wikipedia Article Traffic Logs (Amazon Public Data Set)":http://developer.amazonwebservices.com/connect/entry.jspa?externalID=2596
* "Trendingtopics code on github":http://github.com/datawrangling/trendingtopics/tree/master

h3. Running locally in development mode

Fetch the trendingtopics source code:

<pre>
git clone git://github.com/datawrangling/trendingtopics.git
</pre>

Navigate to the root of the source code directory and create the needed configuration files from the provided examples:

<pre>
	$ cd trendingtopics
	$ cp config/config.yml.example config/config.yml
	$ cp config/database.yml.example config/database.yml	
</pre>

Do the normal rails gem install dance for any missing dependencies.

<pre>
	$ rake gems:install
</pre>

We also used the following plugins (already included in /vendor):

* autocomplete
* annotated-timeline
* gc4r (modified a bit)


Create the database:

<pre>
    $ rake db:create
    $ rake db:migrate
</pre>

Populate the app with demo data from 100 wiki articles:

<pre>
    $ rake db:develop
</pre>

Launch the rails app itself 

<pre>
	$ script/server 
	=> Booting Mongrel
	=> Rails 2.3.2 application starting on http://0.0.0.0:3000
	=> Call with -d to detach
	=> Ctrl-C to shutdown server
</pre>
	
Navigate to http://localhost:3000/ to access the application


h3. Deploying the Rails app to EC2

Fetch the source code as shown above, then install the ec2onrails gem as described at http://ec2onrails.rubyforge.org/:

<pre>	$ sudo gem install ec2onrails </pre>
Find AMI id of the latest 32 bit ec2onrails image (in our case this was ami-5394733a):
<pre>	$ cap ec2onrails:ami_ids</pre>

Launch an instance of the latest ec2onrails ami and note the returned instance address from ec2-describe-instances, it will be something like ec2-12-xx-xx-xx.z-1.compute-1.amazonaws.com 

<pre>
	$ ec2-run-instances ami-5394733a -k gsg-keypair
	$ ec2-describe-instances
</pre>

Create the needed configuration files from the provided examples and edit them, filling in your instance address information, keypairs, and other configuration information as indicated in the comments of each file. See the ec2onrails documentation or source code for more details on each setting.  If you want to make changes to the elasticwulf code, be sure to replace the base github repository in deploy.rb and config.yml with your own github location.

<pre>
	$ cp config/deploy.rb.example config/deploy.rb
	$ cp config/s3.yml.example config/s3.yml
	$ cp config/config.yml.example config/config.yml
	$ cp config/database.yml.example config/database.yml	
</pre>	

Be sure to substitute in your own AWS key and secret key in both config.yml and s3.yml (You can leave these out and ec2onrails will still work, it just won't back up MySQL or the log files)

<pre>
	aws_secret_access_key: YYVUYVIUBIBI
	aws_access_key_id: BBKBBOUjbkj/BBOUBOBJKBjbjbboubuBUB
</pre>

If you uncomment the the auth filter in the main page controller, also replace the admin user name and password in config.yml:

<pre>
	admin_user: REPLACE_ME
	admin_password: REPLACE_ME
</pre>

Deploy the app to your launched EC2 instance with Capistrano (this wil take several minutes)

<pre>
    $ cap ec2onrails:setup
    $ cap deploy:cold
</pre>

If you have auth enabled, you can use the admin login information you set in config.yml to access the dashboard from a web browser or as web service at the url of the instance you provided in deploy.rb: http://ec2-12-xx-xx-xx.z-1.compute-1.amazonaws.com . You can also ssh into your running EC2 instance as usual with your keypairs to debug any issues.  See the ec2onrails forums for more help debugging deployment issues.

To redeploy the app after making changes to the base trendingtopics code, just do the usual cap deploy:

<pre>
    $ cap deploy
</pre>

To manually restart the apache service or mongrels:

<pre>
    $ cap ec2onrails:server:restart_services
    $ cap deploy:restart
</pre>

No data will be populated in the production deployed app until you run the included Hadoop Jobs.  To test the deployment, you can use Capistrano to run the db:develop task on the EC2 server, just wipe the dev data before loading real production data.

